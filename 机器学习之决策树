ID3 Iterative dichotomiser /dai'kɔtəmaizer   迭代分类器三代（注意这里分类不一定是二分类，不能因为老师上课给的二分类例子就认为模型是二叉树） 
ID3模型是多叉树模型 可以参考西瓜书的例子
利用信息增益
C4.5 C = Classifier Ross Quinlan 只能重新命名算法因为ID4 ID5等被研究者相继占用
C4.5 与ID3 的3点不同：
  1）用信息增益比率来衡量
  2）可以处理连续属性值
    步骤是  
       把需要处理的样本(对应根节点)或样本子集(对应子树)按照连续变量的大小从小到大进行排序.
       假设该属性对应的不同的属性值一共有N个,那么总共有N−1个可能的候选分割阈值点,每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点,
              根据这个分割点把原来连续的属性分成bool属性.实际上可以不用检查所有N−1个分割点,具体请看下面的例子.
              
              收入(百)	40	48	60	72	80	90
              类别	   否	否	 是	 是	是	 否
        
       分割点应该是 48 和 60 之间 以及 80 和 90之间 （注意：由于需要排序和扫描,会使C4.5的性能有所下降.）
       用信息增益比率选择最佳划分.  
    3）C4.5还能对缺失值进行处理,处理的方式通常有三种:  (西瓜书中有具体如何处理缺失值的例子)
        赋上该属性最常见的值
        根据节点的样例上该属性值出现的情况赋一个概率,比如该节点上有10个样本,其中属性A的取值有6个为是,4个为否.
            那么对改节点上缺失的属性A,以0.6的概率设为是,0.4的概率设为否.
        丢弃有缺失值的样本
        
CART Classification and regression tree 分类回归树
理解什么是在CART树中特征可以被重复利用：因为CART树是二叉树 如果一个离散特征需要多叉的时候，就只能对于这个特征二叉之后再二叉，这就叫重复利用

上面是三棵单独的树,boosting可以将三棵树集合起来然后生成一棵树 分而治之的思想 由最开始不好的模型，逐渐累加 修正成一个比较好的模型

BDT boosting decision tree -> boosting 里面的基函数是决策树
