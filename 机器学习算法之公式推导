学习图谱
1)线性回归线路：
    广义线性回归 线性回归 logistic回归 人工神经网络 CNN RNN->LSTM GAN
               感知机  SVM
2)决策树线路:
    ID3 C4.5 CART     BDT -> GBDT(Gradient BDT)
    RF <- Bagging Boosting AdaBoost
3)贝叶斯线路:
    朴素贝叶斯 Naive Bayes  HMM(Hidden Markov Model)->CRF(Condition Random Field)
    期望最大->GMM(高斯混合模型)
    K均值->密度聚类->层次聚类

最基本的五个公式推导：线性回归、logistic回归、决策树、贝叶斯、SVM 

难点1）线性回归推导中 E(x) = (y-wx)'(y-wx)对w的求导过程 这里面需要用的矩阵求导的公式有
     矩阵的迹求导  dE/dx=tr(dE'/dx) tr(A)=tr(A')
难点2) SVM中几个点
     首先明白函数间隔和几何间隔的定义 函数间隔是怎么引出的 y(wx+b) 
     其次理解拉格朗日函数 求 min f(w,b) 约束条件为 c(w,b)<=0 h(w,b)=0 最终转化为 
        min(w,b) max(m,n) l = f(w,b)+m*c(w,b)+n*h(w,b) m,n为拉格朗日乘子
        两种理解方法：1）惩罚项 把 m*c(w,b)+n*h(w,b) 想象为对函数的惩罚，我们要求l最小值，当不满足约束条件时 函数会无穷大
                   2）可以证明满足约束条件时 max(m,n) l 的最小值 = l的最小值
     然后理解拉格朗日对偶 在满足KKT条件下 min(w,b) max(m,n) l  问题转化为  max(m,n) min(w,b) l 问题
         理解KKT条件 凸优化函数f(x)中有最小点记为x_ 在该点满足约束条件并且 l'(x_)=0 
     
