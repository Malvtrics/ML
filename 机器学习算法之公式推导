学习图谱
1)线性回归线路：
    广义线性回归 线性回归 logistic回归 人工神经网络 CNN RNN->LSTM GAN
               感知机  SVM
2)决策树线路:
    ID3 C4.5 CART     BDT -> GBDT(Gradient BDT)
    RF <- Bagging Boosting AdaBoost
3)贝叶斯线路:
    朴素贝叶斯 Naive Bayes  HMM(Hidden Markov Model)->CRF(Condition Random Field)
    期望最大->GMM(高斯混合模型)
    K均值->密度聚类->层次聚类

最基本的五个公式推导：线性回归、logistic回归、决策树、贝叶斯、SVM 

难点1）线性回归推导中 E(x) = (y-wx)'(y-wx)对w的求导过程 这里面需要用的矩阵求导的公式有
     矩阵的迹求导  dE/dx=tr(dE'/dx) tr(A)=tr(A')
难点2) SVM中几个点
     首先明白函数间隔和几何间隔的定义 函数间隔是怎么引出的 y(wx+b) 
     其次理解拉格朗日函数 求 min f(w,b) 约束条件为 c(w,b)<=0 h(w,b)=0 最终转化为 
        min(w,b) max(m,n) l = f(w,b)+m*c(w,b)+n*h(w,b) m,n为拉格朗日乘子
        两种理解方法：1）惩罚项 把 m*c(w,b)+n*h(w,b) 想象为对函数的惩罚，我们要求l最小值，当不满足约束条件时 函数会无穷大
                   2）可以证明满足约束条件时 max(m,n) l 的最小值 = l的最小值
     然后理解拉格朗日对偶 在满足KKT条件下 min(w,b) max(m,n) l  问题转化为  max(m,n) min(w,b) l 问题
         理解KKT条件 凸优化函数f(x)中有最小点记为x_ 在该点满足约束条件并且 l'(x_)=0 
     核函数理解： 为什么用核函数？ 在线性不可分情况下需要把数据从低维空间映射到高维空间 注意上面用拉格朗日方法求出的结果中x是内积形式
         举例 原2维空间两个向量(a1,b1) (a2,b2) 映射到 5维空间 则可以发现映射后向量的内积
          （a1,a1^2,b1,b1^2,a1b1)·(a2,a2^2,b2,b2^2,a2b2)=a1a2+(a1a2)^2+b1b2+(b1b2)^2+a1a2b1b2
         我们找到一个函数((a1,b1)·(a2,b2)+1)^2 结果和上述结果相似，由此直接解决了在用低维向量直接计算高维向量内积的问题
