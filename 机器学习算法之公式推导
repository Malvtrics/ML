学习图谱
1)线性回归线路：
    广义线性回归 线性回归 logistic回归 人工神经网络 CNN RNN->LSTM GAN
               感知机  SVM
2)决策树线路:
    ID3 C4.5 CART     BDT -> GBDT(Gradient BDT)
    RF <- Bagging Boosting AdaBoost
3)贝叶斯线路:
    朴素贝叶斯 Naive Bayes  HMM(Hidden Markov Model)->CRF(Condition Random Field)
    期望最大->GMM(高斯混合模型)
    K均值->密度聚类->层次聚类

最基本的五个公式推导：线性回归、logistic回归、决策树、贝叶斯、SVM 

难点1）线性回归推导中 
        #写成 (y-wx)'(y-wx)形式 
        #取y'xw
        #满秩或者正定矩阵理解 行列维度线性相关与否 数学理解：一个向量经过它的变化后的向量与其本身的夹角小于等于90度
难点2) SVM中几个点
     首先明白函数间隔和几何间隔的定义 函数间隔是怎么引出的 y(wx+b) 
     其次理解拉格朗日函数 求 min f(w,b) 约束条件为 c(w,b)<=0 h(w,b)=0 最终转化为 
        min(w,b) max(m,n) l = f(w,b)+m*c(w,b)+n*h(w,b) m,n为拉格朗日乘子
        两种理解方法：1）惩罚项 把 m*c(w,b)+n*h(w,b) 想象为对函数的惩罚，我们要求l最小值，当不满足约束条件时 函数会无穷大
                   2）可以证明满足约束条件时 max(m,n) l 的最小值 = l的最小值
                   https://www.cnblogs.com/90zeng/p/Lagrange_duality.html
     然后理解拉格朗日对偶 在满足KKT条件下 min(w,b) max(m,n) l  问题转化为  max(m,n) min(w,b) l 问题
         理解KKT条件 凸优化函数f(x)中有最小点记为x_ 在该点满足约束条件并且 l'(x_)=0 
     核函数理解： 为什么用核函数？ 在线性不可分情况下需要把数据从低维空间映射到高维空间 注意上面用拉格朗日方法求出的结果中x是内积形式
         举例 原2维空间两个向量(a1,b1) (a2,b2) 映射到 5维空间 则可以发现映射后向量的内积
          （a1,a1^2,b1,b1^2,a1b1)·(a2,a2^2,b2,b2^2,a2b2)=a1a2+(a1a2)^2+b1b2+(b1b2)^2+a1a2b1b2
         我们找到一个函数((a1,b1)·(a2,b2)+1)^2 结果和上述结果相似，由此直接解决了在用低维向量直接计算高维向量内积的问题
           后面有机会实现一下 高斯核函数 理解其中 参数带来的影响 面试100题P54 
        https://blog.csdn.net/qq_40027052/article/details/78773302
        SVM 理解和推导
        https://blog.csdn.net/v_july_v/article/details/7624837
        几何间隔 等于 函数间隔 除以法向量模长 具体公式自己推导
        应该用最大几何间隔分类超平面
        让函数间隔等于1 之所以令等于1，是为了方便推导和优化，且这样做对目标函数的优化没有影响，至于为什么，请见本文评论下第42楼回复

        拉格朗日乘子、对偶以及KKT的理解，看懂这个下面这个文章的推导，并且能够手动推导
        https://www.cnblogs.com/maybe2030/p/4946256.html

KL散度(相对熵)的计算公式和理解
在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]
直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，
需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q等价于P
KL(p||q)= ∑i=1^n p(xi)log(p(xi)/q(xi))

交叉熵是相对熵拆开之后的后半部分
下面链接有个实例
https://blog.csdn.net/tsyccnh/article/details/79163834
