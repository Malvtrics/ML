word2vec是干嘛的？？？ 
完成单词的向量化表示
找到这些带有语义的词向量
语义相近的词向量距离会接近 从这个角度来讲当前一些模型的效果 bert>cbow/skip-gram>not-hot

概念

单词
词典D     {w_1,w_2...w_N} 由N个单词组成的集合 注意 无序 不重复
语料库C   由单词构成的文本序列 注意 有序 可以重复 
上下文    以w_t为中心词的上下文（前c个和后c个）表示为 context(w_t) = {w_{t-c}...w_t...w_1} 

两个模型 

w_t 决定 context(w_t) 为词袋模型 CBOW continus bag of words
context(w_t) 决定w_t  为跳字模型 skip_gram

两种方法 
层序softmax hierachical softmax
下采样方法 negative sampling

训练集可以看做语料库的真子集

基于层序softmax的CBOW
context(w_t)_1 context的第一个元素，也就是w_t的第前c个单词
v(context(w_t)_1) 通过v函数把这个单词转化为一个向量 比如用one-hot编码方法 当然这只是一种方法，可以采用其他的
这里假设向量是m维的

输入层 v(context(w_t)_1) v(context(w_t)_2) v(context(w_t)_3) ...  v(context(w_t)_2c)
映射层 x_w = 将2c个向量求和 得到一个还是m维的向量 （理解为什么求和 这一步集合了w_t上下文的所有信息）
输出层 找到了对应的中心词w_t 也就意味着在词典中其他的单词没有被选择 所以这其实一个分类问题 
       对每一个字典中的单词是否被选择都要给出一个清晰的结论 可以是0、1 也可以是概率
       所以最终输出是一个N维的向量 （多分类问题 所以可用softmax解决）
       但是如果用softmax， 当N很大的时候不方便 所以引出 层序softmax => hiarachical softmax
       
       这里其实思想将数据结构 从 数组 到 树  构建哈夫曼二叉树 减小搜索路径 N个单词分别落在N个叶子节点上
       时间复杂度 从数量级N 到 logN
       
       那么哈夫曼二叉树是一颗带权路径二叉树，怎么定义权重呢？ 可以根据单词在语料库中出现的频率大小来定义
       但是这里注意要去除停止词
       
       那么问题转化为从根节点出发分类最后找到对应叶子节点的过程 二分类用对数损失即可
       后面就是最小化损失函数的问题 套路一样的
       
       注意似然函数是对整体概率的累乘，就是说要包括语料库中所有的单词，上面只是对某一个单词做分析
       
基于层序softmax的skip_gram
只不过没有映射层，因为输入只有一个中心单词
但是哈夫曼树中，要求在这个中心单词下，得到上下文2c个单词的概率
p(context(w)|w) = 求和2c个 context(w)|w

下采样思想：也是要降低计算而复杂度N ， 因为最终输出的一个概率向量，可以考虑从字典中只取一部分词，而不是整个N
损失函数等处理过程和上面类似，我们主要关注如何下采样

简单的思路是随机采样
但是这样不太公平，因为单词的重要性不同，所以用带权采样
权重计算方法：根据单词在预料库中出现的次数除以所有单词在语料库中出现的次数

所有单词权重相加势必为1，把这条长度为1的线段等分为很小的段，随机抽取其中一段，然后抽取这一小段对应的单词，就完成了负采样
