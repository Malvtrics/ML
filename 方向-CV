计算机视觉：让计算机理解图像的内容
2012年imagenet alexnet出现之后 CNN 开始蓬勃发展
flir forward  looking infra-red 前视红外雷达
里程碑的几个模型

RCNN 
OverFeat 
Fast R-CNN
RPN
Faster R-CNN
YOLO1
SSD 
YOLO V2
RetinaNet
Mask R-CNN
YOLO V3

两个指标 IoU 和 AP 交并比和平均准确率
IoU Intersection over union 交并比

how to get the AP
1) plot the recall the recall value increases as we in clude more prediction but the precision will go up and down 
2) we smooth out the zigzag pattern first (变成横平竖直的折线)
3）we divide the recall value from 0 to 1 into 11 points and compute teh average of maximum precision value for the 11 recall values

1：首先是对于图像的边缘检测，可能要看opencv的源码
2：理解从 r-cnn fast-cnn faster-cnn mask-cnn的一个演变的路线和各个阶段所解决的问题
3：理解图像处理中的双线性插值 其实挺简单的  https://blog.csdn.net/qq_37577735/article/details/80041586
4：锚点anchor很重要，好想就这个看明白了，就是为了减少模型训练的时候的参数量，对于每个feature map, 设置9个anchor 
   3组长宽高 每组包括一个正方形和横纵两个长方形

YOLO算法 you only look once
在《基于深度学习的目标检测思路》中，提到了可以用滑动窗口的思路来做目标检测。除了滑动窗口，还有其他的目标检测算法吗？
该方法对目标的标注，需要标注目标的位置、大小、类型等信息，标注成本是很高的。但是，做目标检测是少不了这个标注工作的。
这种传统的滑动窗口目标检测方法，最大的缺点是： 窗口大小不固定，需要动态改变窗口做多次训练、预测操作，这对模型的训练是非常耗时的。

有没有办法针对一幅图像，只做一次训练呢？这就是YOLO算法要解决的问题。
https://blog.csdn.net/ybdesire/article/details/81227424
网格覆盖原图后，就对每个小网格的内容进行标注，需要标注如下信息

pc：小网格内是否有目标物体的中心点，注意是根据中心点进行标注，横跨多个网格的物体也只标注有中心点存在的网格。
bx,by,bh,bw：目标物体中心点在网格中的x-y坐标，以及目标物体的长、宽。一般将小网格长宽取1后进行标注，这样位置坐标都被转化为0~1之间的数值。
c1，c2，c3：类别，这里假设分三类。

https://github.com/pjreddie/darknet/wiki/YOLO:-Real-Time-Object-Detection

SSD single shot multibox detector
1-> let's wheat single shot detector
single shot detector using VGG19 as a feature extractor 
image->VG19->conv->conv->conv->conv->conv(five conv layers)+conv(convolution filters)->classes + boundary boxes
convolution layers reduce spatial dimentsion and resolution
so the model above can detect large objects only , to fix that we make independent object detections from multiple feature maps

VGG19 https://www.jianshu.com/p/6aa903648ec5 了解下

SSD经过VGG后 38*38 a pretty large reduction from the input image, hence SSD usualy perfoms badly for small objects comparing with other 
dection methods. 我们可以通过提高原图像分辨率的方法来解决这个问题

模型的两种损失
1)localization loss 
the localization loss is the mismatch between the ground truth box and the predicted boundary box 
SSD only penalizes predictions from positive mathces (首先保证是一个object 如果连个物体都不是，就不用计loss)
to get coloser to the ground truth. Negtive matches can be ignored (不是object忽略)
2)confidence loss 分类损失
the confidence loss is the loss in making a class prediction 
for every positive match prediction, we penalize the loss according to the confidence score of the corresponding class

hard negative mining 难例挖掘
there are much more negative matches than positive mathces 
this creates a class imbalance which hurts training 
instead of using all the negatives , we sort those negatives by their calculated confidence loss
SSD picks the negatives with the top loss and makes sure the ratio between the picked negatives and positives is at most 3:1
this leads to a faster and more stable training

data augmentation
each training image is randomly sampled by one of the following options:
2)use the original
3)sample a pitch with IoU of 0.1 0.3 0.5 0.7 0.9
3)randomly sample a patch
the sampled patch will have an aspect ratio between 1/2 and 2 
aspect ratio: 图像的宽高比
Then it is resized to a fixed size and we flip one-half of the training data. 
In addtion, we can apply photo distortion

several points:
1) SSD performs worse than faster R-CNN for small-scale objects 
   in SSD small objects can only be detected in higher resolution layers (left most layers)
   but those layers contains low-level features like edges or color patches, that are less infromative for classification
2) accuracy increases with the number of default boundary boxes at the cost of speed
3) multi-scale feature maps improve the detection of objects at different scale(VGG带来了feature map的上限-》后面理解下？？？)
4) design better default boundary boxes will help accuracy
5) SSD has lower localization error comparing with RCNN but more classification error dealing with similar categories
   the higher classification errors are likely because we use the same boundary box to make multiple class predictions
6) SSD512(512指的是分辨率) has better accuracy 2.5% than SSD300 but run at 22 FPS instead of 59

by removing the delegated region proposal and using lower resolution images , the model can run at real-time speed and still beats 
the accuracy of the state-of-the-art(应用最先进技术的) faster RCNN

YOLO-V3结构图看得云里雾里啊~~可能需要看一下论文
经过多层的卷积  分辨率变小 感受野变大
downsampling 的倍数越大 说明resolution越小 说明感受野越大 越不容易检测出小物体

图像下采样就是缩小图像
上采样常用的方法就是内插值方法


小的feature map是用来检测大的物体， 大的feature map用来检测小的物体
理解：一张图像分的格子越小，每个格子面积大，用来检测大物体

YOLO-V2分类采用的是MSE 
YOLO-V3分类损失用的交叉熵，这个比较make sense 
softmaxing classes rest on the assumption that classes are mutually exclusive
this works fine in COCO dataset (Microsoft Common Objects in Context 微软2014出资标注的数据集)
however when we have classes like Person and Women in a dataset the above assumption fails.
This is the reason why the authors of YOLO have refrained(克制 节制 避免) from softmaxing the classes
instead each class score is predicted using LR and threshold is used to predict multiple labels from an object

clustering for anchor boxes:
in many problem domains, the boundary boxes ave strong patterns,
for example, in the autonomous driving, the most common boundary boxes will be cars and pedistrians（行人）at different distances
to identify the top-k boundary boxes that have the best coverage for the training data, we run K-Means clustering on the data
to locate the centroids of the top-k clusters

to make a decision on how many anchor boxes to use, we plot the curve IoU vs N cluster plot
the ides is that the true number of clusters is captured when the increase in the mean IoU slope 开始很大后面逐渐平缓
in this case I would say that 4 anchor boxes may be a good size 
anchor box 越多，准确率会上去但是效率会下降，折中取4个

什么是focal loss?

focal loss 解决什么问题？ 样本数量不均衡问题  
positive sample >>>>> negative samples (class imbalance)
easy samples >>>>> hard samples 

用交叉熵损失的时候 alpha代表平衡因子 用来解决正负样本不均衡  gamma用来解决难易样本不均衡
详细看 预习视频 2-深度学习在物体检测中的应用 CV方向 1:17:46

focal loss调整了权重，对于那些分类已经很正确的样本，权重会下降，对于分类错误的样本，权重会升高
实际应用中 gamma=2 和 alpha=0.25的组合常用


VGG: Faster RCNN/SSD
MobileNet: Faster RCNN/R-FCN/SSD
InceptionV2 : Faster RCNN/R-FCN/SSD
Resnet : Faster RCNN/R-FCN/SSD
Inception Resnet V2 : Faster RCNN/R-FCN/SSD

mAP(mean average precision) 
准确率和召回率单独来评价模型不科学，结合这两个指标的话可以用F1度量，
其实还有一种方法，即Average Precision（平均精度AP）——以召回率和准确率为行纵坐标，得到二维曲线，即PR曲线。
将PR曲线下的面积当作衡量尺度，得到AP值。mAP即平均AP值,是对多个验证集求平均AP值

注意AUC 和 AP 区别 横纵坐标不同


The difference between detectors is narrowing , 
single shot uses more complex designs to make it more accurate 
region based detectors stramline the operaion to make it faster
Eventually the significant diff may not be in the basic concept of the models but on the implementation details 
1) Feature pyramid networks produces semantic rich feature maps with high resolution object spatial information to improve accuracy
https://blog.csdn.net/qq_17550379/article/details/80375874
2）complex feature extractors like ResNet and Inception ResNet are key to high accuracy if speed is not a concern
3) single shot detecotrs with light but powerful extractor like MobileNet is good for realtime processing
   in particular for less powerful mobile device
4) use batch normalization
5) experiment different feature extractors to find a good balance between speed and accuracy
   some light weight extractors make significant speed improvement with tolerable accuracy drop
6) use anchors to make boundary box predictions 
7) select anchors carefully 
8) crop images in traning to learn features in diff scales
9) at the cost of speed , higher resolution input images improves accuracy, in particular for small obj
10) fewer proposals for faster RCNN can imporve speed without to much accuracy drop
11) end to end traning with multi-task loss improves performance
12) experiment diff weights for diff losses
13) experiment atrous mode (https://www.zhihu.com/question/49630217 带洞卷积)
   it provides wider field of view at the same computational cost. it can help accuracy
14)exp the number of proposals or predictions per grid cell

https://zhuanlan.zhihu.com/c_1178388040400302080
   








