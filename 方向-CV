计算机视觉要处理的问题：让计算机理解图像的内容

Alexnet是2012年Imagenet竞赛的冠军模型，准确率达到了57.1%, top-5识别率达到80.2%。此后CNN开始蓬勃发展

几个里程碑模型
RCNN 
OverFeat 
Fast R-CNN
RPN
Faster R-CNN
YOLO V1 (you look only once)
SSD 
YOLO V2
RetinaNet
Mask R-CNN
YOLO V3

两个指标 IoU(Intersection over union 交并比) 和 AP (平均准确率) 
两个指标的关系：iou高 准确率高 iou低 Boundingbox多 召回率高

在多标签图像分类问题中 AP指标变为MAP
参考一篇非常好地介绍MAP的文章，解答了自己心中的疑惑 https://www.jianshu.com/p/82be426f776e
其实简单理解这个算法就是：对于多标签图像分类问题假设有A个标签 以某个标签（比如是否是车辆）
一共有N个样本，其中车辆有M个
将所有样本通过模型的预测值从大到小排序，选取TOP-N作为评判依据，随着N越大，召回率越高而准确率越低，直到所有样本结束
可以画出一个召回率和准确率的图，其中召回率一定有M个，找出每个召回率对应的最大的准确率，然后求平均 就得到这个标签对应的AP
将A个标签所有的AP求平均就可以得到MAP
文中有一句话说得很好： MAP是为解决Precision，Recall，F-measure的单点值局限性的。为了得到 一个能够反映全局性能的指标

1：首先是对于图像的边缘检测，可能要看opencv的源码
2：理解从 r-cnn fast-cnn faster-cnn mask-cnn的一个演变的路线和各个阶段所解决的问题
3：理解图像处理中的双线性插值 其实挺简单的  https://blog.csdn.net/qq_37577735/article/details/80041586
4：锚点anchor很重要，好想就这个看明白了，就是为了减少模型训练的时候的参数量，对于每个feature map, 设置9个anchor 
   3组长宽高 每组包括一个正方形和横纵两个长方形

YOLO模型
在《基于深度学习的目标检测思路》中，提到了可以用滑动窗口的思路来做目标检测。除了滑动窗口，还有其他的目标检测算法吗？
该方法对目标的标注，需要标注目标的位置、大小、类型等信息，标注成本是很高的。但是，做目标检测是少不了这个标注工作的。
这种传统的滑动窗口目标检测方法，最大的缺点是： 窗口大小不固定，需要动态改变窗口做多次训练、预测操作，这对模型的训练是非常耗时的。

有没有办法针对一幅图像，只做一次训练呢？这就是YOLO算法要解决的问题。
https://blog.csdn.net/ybdesire/article/details/81227424
网格覆盖原图后，就对每个小网格的内容进行标注，需要标注如下信息

pc：小网格内是否有目标物体的中心点，注意是根据中心点进行标注，横跨多个网格的物体也只标注有中心点存在的网格。
bx,by,bh,bw：目标物体中心点在网格中的x-y坐标，以及目标物体的长、宽。一般将小网格长宽取1后进行标注，这样位置坐标都被转化为0~1之间的数值。
c1，c2，c3：类别，这里假设分三类。

https://github.com/pjreddie/darknet/wiki/YOLO:-Real-Time-Object-Detection

SSD single shot multibox detector
1-> let's wheat single shot detector
single shot detector using VGG19 as a feature extractor 
image->VG19->conv->conv->conv->conv->conv(five conv layers)+conv(convolution filters)->classes + boundary boxes
convolution layers reduce spatial dimentsion and resolution
so the model above can detect large objects only , to fix that we make independent object detections from multiple feature maps

VGG19 https://www.jianshu.com/p/6aa903648ec5 了解下

SSD经过VGG后 38*38 a pretty large reduction from the input image, hence SSD usualy perfoms badly for small objects comparing with other 
dection methods. 我们可以通过提高原图像分辨率的方法来解决这个问题

模型的两种损失
1)localization loss 
the localization loss is the mismatch between the ground truth box and the predicted boundary box 
SSD only penalizes predictions from positive mathces (首先保证是一个object 如果连个物体都不是，就不用计loss)
to get coloser to the ground truth. Negtive matches can be ignored (不是object忽略)
2)confidence loss 分类损失
the confidence loss is the loss in making a class prediction 
for every positive match prediction, we penalize the loss according to the confidence score of the corresponding class

hard negative mining 难例挖掘
there are much more negative matches than positive mathces 
this creates a class imbalance which hurts training 
instead of using all the negatives , we sort those negatives by their calculated confidence loss
SSD picks the negatives with the top loss and makes sure the ratio between the picked negatives and positives is at most 3:1
this leads to a faster and more stable training

data augmentation
each training image is randomly sampled by one of the following options:
2)use the original
3)sample a pitch with IoU of 0.1 0.3 0.5 0.7 0.9
3)randomly sample a patch
the sampled patch will have an aspect ratio between 1/2 and 2 
aspect ratio: 图像的宽高比
Then it is resized to a fixed size and we flip one-half of the training data. 
In addtion, we can apply photo distortion

several points:
1) SSD performs worse than faster R-CNN for small-scale objects 
   in SSD small objects can only be detected in higher resolution layers (left most layers)
   but those layers contains low-level features like edges or color patches, that are less infromative for classification
2) accuracy increases with the number of default boundary boxes at the cost of speed
3) multi-scale feature maps improve the detection of objects at different scale(VGG带来了feature map的上限-》后面理解下？？？)
4) design better default boundary boxes will help accuracy
5) SSD has lower localization error comparing with RCNN but more classification error dealing with similar categories
   the higher classification errors are likely because we use the same boundary box to make multiple class predictions
6) SSD512(512指的是分辨率) has better accuracy 2.5% than SSD300 but run at 22 FPS instead of 59

by removing the delegated region proposal and using lower resolution images , the model can run at real-time speed and still beats 
the accuracy of the state-of-the-art(应用最先进技术的) faster RCNN

YOLO-V3结构图看得云里雾里啊~~可能需要看一下论文
经过多层的卷积  分辨率变小 感受野变大
downsampling 的倍数越大 说明resolution越小 说明感受野越大 越不容易检测出小物体

图像下采样就是缩小图像
上采样常用的方法就是内插值方法


小的feature map是用来检测大的物体， 大的feature map用来检测小的物体
理解：一张图像分的格子越小，每个格子面积大，用来检测大物体

YOLO-V2分类采用的是MSE 
YOLO-V3分类损失用的交叉熵，这个比较make sense 
softmaxing classes rest on the assumption that classes are mutually exclusive
this works fine in COCO dataset (Microsoft Common Objects in Context 微软2014出资标注的数据集)
however when we have classes like Person and Women in a dataset the above assumption fails.
This is the reason why the authors of YOLO have refrained(克制 节制 避免) from softmaxing the classes
instead each class score is predicted using LR and threshold is used to predict multiple labels from an object

clustering for anchor boxes:
in many problem domains, the boundary boxes ave strong patterns,
for example, in the autonomous driving, the most common boundary boxes will be cars and pedistrians（行人）at different distances
to identify the top-k boundary boxes that have the best coverage for the training data, we run K-Means clustering on the data
to locate the centroids of the top-k clusters

to make a decision on how many anchor boxes to use, we plot the curve IoU vs N cluster plot
the ides is that the true number of clusters is captured when the increase in the mean IoU slope 开始很大后面逐渐平缓
in this case I would say that 4 anchor boxes may be a good size 
anchor box 越多，准确率会上去但是效率会下降，折中取4个

什么是focal loss?

focal loss 解决什么问题？ 样本数量不均衡问题  
positive sample >>>>> negative samples (class imbalance)
easy samples >>>>> hard samples 

用交叉熵损失的时候 alpha代表平衡因子 用来解决正负样本不均衡  gamma用来解决难易样本不均衡
详细看 预习视频 2-深度学习在物体检测中的应用 CV方向 1:17:46

focal loss调整了权重，对于那些分类已经很正确的样本，权重会下降，对于分类错误的样本，权重会升高
实际应用中 gamma=2 和 alpha=0.25的组合常用


VGG: Faster RCNN/SSD
MobileNet: Faster RCNN/R-FCN/SSD
InceptionV2 : Faster RCNN/R-FCN/SSD
Resnet : Faster RCNN/R-FCN/SSD
Inception Resnet V2 : Faster RCNN/R-FCN/SSD

mAP(mean average precision) 
准确率和召回率单独来评价模型不科学，结合这两个指标的话可以用F1度量，
其实还有一种方法，即Average Precision（平均精度AP）——以召回率和准确率为行纵坐标，得到二维曲线，即PR曲线。
将PR曲线下的面积当作衡量尺度，得到AP值。mAP即平均AP值,是对多个验证集求平均AP值

注意AUC 和 AP 区别 横纵坐标不同


The difference between detectors is narrowing , 
single shot uses more complex designs to make it more accurate 
region based detectors stramline the operaion to make it faster
Eventually the significant diff may not be in the basic concept of the models but on the implementation details 
1) Feature pyramid networks produces semantic rich feature maps with high resolution object spatial information to improve accuracy
https://blog.csdn.net/qq_17550379/article/details/80375874
2）complex feature extractors like ResNet and Inception ResNet are key to high accuracy if speed is not a concern
3) single shot detecotrs with light but powerful extractor like MobileNet is good for realtime processing
   in particular for less powerful mobile device
4) use batch normalization
5) experiment different feature extractors to find a good balance between speed and accuracy
   some light weight extractors make significant speed improvement with tolerable accuracy drop
6) use anchors to make boundary box predictions 
7) select anchors carefully 
8) crop images in traning to learn features in diff scales
9) at the cost of speed , higher resolution input images improves accuracy, in particular for small obj
10) fewer proposals for faster RCNN can imporve speed without to much accuracy drop
11) end to end traning with multi-task loss improves performance
12) experiment diff weights for diff losses
13) experiment atrous mode (https://www.zhihu.com/question/49630217 带洞卷积)
   it provides wider field of view at the same computational cost. it can help accuracy
14)exp the number of proposals or predictions per grid cell

https://zhuanlan.zhihu.com/c_1178388040400302080
   

triplet loss 
CNN是通过softmax来解决排序的问题，想想是不是有哪里不太一样？
对于某一个神经网络，我的输入是带有三个输
怎么理解embedding??
https://www.zhihu.com/question/38002635

Euclidean space 欧式空间
harmonic embeddings 谐波嵌入
off the shelf 现成的
bottleneck layer 瓶颈层 输入输出维度差距较大，就像一个瓶颈一样，上窄下宽亦或上宽下窄，深入理解下如下链接
https://blog.csdn.net/u011501388/article/details/80389164

cross level vehicle recongnition CLVR
robotics
myriad 无数的

数据扩增的包 imgaug 数据增强库 网上有对应的学习笔记

如果现在有一个车辆识别任务

method1: train convnets to predict vehicle models
training 就直接用inception v3

start with a convnet (eg.inception v3) initialized with imagenet pretrained weights
remove the lsat fully-connected layers 
extract the 2048-d features by adding a gloable average polling layer after the last convolutional layer
adding the output layer for predicting vehicle models which is a fully-connected layer

裁剪图片的时候 a better way : 因为图片很多情况下是长方形，不能直接选中中心位置，不然不准确，
所以先确定短边，然后按照比例找中间的小长方形，然后已中心位置旋转90度，两个长方形交叠的正方形为最后裁剪结果
we call this method as center-crop 
但是这样的问题是：如果每次选取中心位置，那么信息可能有丢失
为了保证每次都能训练到不同的位置，在实际操作过程中，会用padding去做一些补零的操作，然后每次随机去做移动，这样每次训练都可以得到不同的区域

进阶method2: 利用颜色数据做多任务学习
凡是神经网络有两个以上输出分支的我们call it multi-task learning
这里我们输出车型和颜色两个分支 颜色更加容易学习，所以在实际操作中可以降低权重

进阶method3：multi-task learning = triple loss + cross-entropy

在实际训练中，我们如何准备我们得negative数据呢？
比如一张图片是白色大众帕萨特，它的negative如果是红色奥迪，其实在上面的multi-task learning中
我们已经可以根据颜色分支区别出来，所以我们要选取一些hard negative的输入样本，
比如白色奥迪，这样可以更加高效的利用负样本，让模型可区分的颗粒度更细

结论：你想让模型区分哪些方面？就喂给模型什么样的数据，否则triplet loss没办法发挥作用





----------------------------------
调参工程师 或者 高级 调参工程师

output: 
loss curves 
gradient norms

Architecture：
using or adapt from established networks:
   classification: AlexNET vgg resnet denseNet
   segmentation ...
data is less , then we not need to set the param too large, for small dataset can reduce the channels 
change the last layers due to real task:
1) regression task : change to FC+LR
2)...

when tuning parameters, only use a samll portion of the dataset for fast iteration 
-> because if you use lots of data to train, then every time if you change one param will need long time 
eg: make sure you model overfit curernt small portion (make the loss function can 收敛 quickly)

Class YourAwesomeDataset(torch.data.Dataset):
   def __init__(root_dir,debug=False):
    ***
    
Reading the learning curves:
Loss curves: 
6 pictrues:
1) 来回震荡太厉害,没有收敛 -》比较差的情况，要去检查数据处理阶段问题等等
2) train loss is lower than val loss -> overfit 看最后收敛的百分比，30% 以上可能就要注意 10%到20%可以酌情
   方法 L1 L2 norm dropp out 等等 
3) train loss is lower than val loss -> overfit 比起第二个 
4) train loss and val loss 相伴下降，但是类似一个斜线  最后的结果不是最好的，应该还能到一个值，收敛的斜率不平
   方法：先learning rate 大再小
5) 开始平稳，也要调大学习率
6) 梯度方向错误 不讨论

7) 8) same problem need to shuffle the original data 
9) need normlization
10) train loss lower 

checklist:
overfit on a small dataset
train on standard corpora
mean certering 
balanced dataset 
   offline: 
   online: eg: batch_size = 256 sample from diff classes (if 10) when 11 then sample from class 1,2,3 .. then it will make sure all
           the classes are balanced in the batch
   loss function: control weight 
simplify your model
weight init 
pretrianed model 
learning rate
cross-entropy loss 
regularization loss  first close it -> make the model overfit -> then open it 
action function 
gradient clipping-> make a threshold for the weight , if over, then pull it back 

iteration vs epoch 
iteration -> epoch / batch size 
epoch -> iteration to the whole data set

工业界想牺牲一定的准确性保证高效率

Xception:
depthwise conv on seperate channel 

flir forward  looking infra-red 前视红外雷达



