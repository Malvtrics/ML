强化学习 反馈是有延时的 两个场景 围棋下错一步，影响后面的步数 滴滴给司机派单 影响司机全天的最后收益等

数据驱动 = 数据+模型

数据也很重要 互联网巨头因为有大数据支撑才能让模型训练出很好的效果
模型 
  监督 损失函数+优化
    损失函数 loss function \ cost function \ objective function 一样 => 最小损失函数
    kaggle 比谁的模型更好一点 第一名用了160多个模型
    过拟合的另外一种理解： 两个未知数但是有三个或者更多的方程 （邻居家的天才儿子，哈哈哈）
      超参数 阿尔法控制 GD学习速率
      线性回归 正则化 控制参数的幅度， 不能让模型无法无天 （抖动太厉害，甩起来） （不仅要看标准答案，还要看过程）超参数 拉姆达 =》加多少控制
    分类问题 =》 逻辑回归 
      sigmoid funtion 熟悉它的导数！
      寻找decision boundry 不用之前损失函数的原因是 可能会找到多个极值点，但不一定是最小值 所以定义了一个新的损失函数 （未完待续）
      
理解极大似然

假设要求两枚硬币AB各自抛掷后正面向上的概率PA PB
我们做5次试验
A 正正反正反 
B 反反正正饭
A 正反反反反
B 正反反正正
A 反正正反反
PA=0.4 PB=0.5

但是如果我们不知道每次抛掷的是A还是B，怎么求呢
？ 正正反正反 
？ 反反正正饭
？ 正反反反反
？ 正反反正正
？ 反正正反反

E-step
假设一个初始值PA=0.2 PB=0.7
然后对下面每次的抛掷求 如果是硬币A，出现这种结果的概率 和  如果是硬币B，出现这种结果的概率 
M-step
对比这两个概率，取大一点的（最有可能的那个=这就叫极大似然）作为结果，可以得到
B 正正反正反 
A 反反正正饭
A 正反反反反
B 正反反正正
A 反正正反反
通过这个结果算出PA=0.33 PB=0.6 然后用这个算出的结果重复E-step

可以发现PA PB逐渐接近真实值
经过不断的迭代最终可以收敛到真实值，这就是EM算法的思想

对drop out的另外一种理解：还不是很理解 哈哈
one major issue in learning large networks is co-adaptation
In such a network, if all the weights are learned together it is common that 
some of the connections will have more predictive capability than the others. 
Over many iterations,only a fraction of the node connections is trained, and the rest stop participating

the loss is decreasing more or less linearly, indicating learning rate may be too low
the loss flucuates a lot, suggesting batch size maybe too small
there is no gap between  the training  and validation accuracy, we should increase the training size

推荐系统中的 wide and deep learning:
wide linear model => memorization pigeons can fly!  seagulls can fly!
deep model => generalization Animals with wing can fly!
我们期待的是 wide linear model + deep model => generalization + memorization exceptions 
=>animals with wings can fly, but penguins can't fly

wide linear models:
memorization can be achieved by adding crossed features to  learn frequent co-current items (哪些app用户经常一起装)
crosses add understandable  non-lineariry to the model 
user feature=> installed app = 途牛  +0.1
impression feature => impression app => 携程 +0.5
crossed feature => AND(installed_app = '途牛',impression_app  = '携程') +1.4

deep  neural networks:
deep  models can generalize prediction to <user,impression> that never or rately co-occured via dense embeddings
hidden layers can learn complex but less interpretable feature interactions
(炸鸡和汉堡看起来没有关联，但是出现在快餐店（理解为一个embedding）的时候，就关联起来了)s

所以借助这两种model的结合得出更好的模型=>后面学下deep FM
