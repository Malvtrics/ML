计算机视觉要处理的问题：让计算机理解图像的内容

------------------------------------------------基本概念部分开始-----------------------------------------------
IoU 
AP(MAP) 
NMS(soft NMS) 
proposal(候选框)
ROI(region of interest)
  从被处理的图像以方框、圆、椭圆、不规则多边形等方式勾勒出需要处理的区域，称为感兴趣区域，ROI。这个区域是你的图像分析所关注的重点。
  圈定该区域以便进行进一步处理。使用ROI圈定你想读的目标，可以减少处理时间，增加精度。

两个指标:IoU(Intersection over union交并比)和AP(平均准确率) 
IoU和precision与recall的关系：IoU->准确率高;IoU低->Boundingbox多->召回率高    => 个人理解也不一定，要看具体情况

关于AP的一些说明：
AP across IoUs:
   for the primary AP 0.5:0.05:0.95 (start:step:end) these would result in computations of AP threshold at ten different IoUs
AP across Scales:
   用boundingbox界定小、中、大的物体(比如用32,96px界定)，然后计算AP看模型对于不同大小物体的预测能力

关于MAP:
在多标签图像分类问题中 AP指标变为MAP
参考一篇非常好地介绍MAP的文章，解答了自己心中的疑惑 https://www.jianshu.com/p/82be426f776e
其实简单理解这个算法就是：对于多标签图像分类问题假设有A个标签 以某个标签（比如是否是车辆）一共有N个样本，其中车辆有M个
将所有样本通过模型的预测值从大到小排序，选取TOP-N作为评判依据，随着N越大，召回率越高而准确率越低，直到所有样本结束
可以画出一个召回率和准确率的图，其中召回率一定有M个，找出每个召回率对应的最大的准确率，然后求平均 就得到这个标签对应的AP
将A个标签所有的AP求平均就可以得到MAP
文中有一句话说得很好： MAP是为解决Precision，Recall，F-measure的单点值局限性的。为了得到 一个能够反映全局性能的指标

关于NMS non-max-suppression非极大值抑制
按照置信度从大到小排序所有的boundingbox,选取最高的box,其他的box和这个box做IoU,超过一定的阈值就扔掉
(理解：和置信度最大的box重叠很多的box其实选出了相同的目标，可以丢弃，box数量减少，准确率上升)
注意这里丢弃、扔掉的意思是：置信度直接置为0

   NMS会产生的问题：回想实例：
   this image has two confident horse detectinos shown in red and green which have a score of 0.95 and 0.8 respectively
   the green detection box has a significant overlap with the red one
   is it better to supperss the green box altogether and assign it a 0 score of a slightly lower score of 0.4?

   如何解决问题？ soft NMS
   不用将置信度置为0而是乘以一个高斯系数，使它比原来小, 目的是不要一篮子丢弃，不然覆水难收

关于proposal候选框，
产生候选框的方式：
1)暴力破解法：sliding-windows-detector不断滑窗
2)selective search:自底向上的不断合并的层次聚类的方法,具体实现可以参考paper文件夹中的对应论文

------------------------------------------------基本概念部分结束-----------------------------------------------

AlexNet是2012年Imagenet竞赛的冠军模型，准确率达到了57.1%, top-5识别率达到80.2%。此后CNN开始蓬勃发展

几个里程碑模型

--RCNN家族-两阶段检测-先拿到proposal再根据特征图像做分类和回归------
RCNN (region-CNN) 2013年提出，利用了AlexNet
Fast R-CNN
RPN
Faster R-CNN
--一阶段检测---没有proposal生成阶段--更加高效适合部署到工业生产环境---
YOLO V1 (you look only once)
SSD 
YOLO V2
RetinaNet
Mask R-CNN
YOLO V3

------------------------------------------------基本模型部分开始----------------------------------------------------------

------------------------------------------------Region Based Object Detectors---------------------------------------------
RCNN 
最终的输出是目标和目标边缘(所在位置)
原始图像
   ->1.Propose category-independent regions by selective search(~2k candidates per image) 
     Those regions may contain target objects and they are of diff sizes
      ->2.Region candidates are warped(改变维度) to have a fixed size as required of CNN(然后将每个候选区域进行尺寸的修整变换)
         ->3.喂给CNN做特征提取(这里的CNN可以是AlexNet VGG ResNet等,AlexNet2012一站成名,所以最初用的这个)
           You can find a pre-trianed AlexNet in Caffe Model Zoo but not in tensorflow
           Tensorflow-slim model libary provides pre-trained ResNet/VGG or others
            ->4.用回归器得到boundarybox+用(binary SVM)确定类别
              to reduce the localization errors , regression model is trained to correct the predicted detection window on bounding box 
              correction offset using CNN features

in step3注意：
1)最后得出K+1类(K是目标物体类别，多出的一类是不带物体的背景图)
2)in the fine-tunning stage,we should use much smaller learning rate and mini-batch oversamples the positive cases
   because most proposed regions are just background
3)Given every image region, one forward propagation through the CNN generates a feature vector. This 
   feature vector is then consumed by a binary SVM trained for each class independently.
   The positive sample are proposed region with IoU overlap threshold >= 0.3

RCNN问题：
1)needs many proposals to be accurate and many regions overlap with each other
2)is slow in training & inference(推理) :
if we have 2000 proposals, each of them is processed by a CNN separately so we repeat feature extraction 2000 times for diff ROIs

如何解决问题?
1)use an extenal region proposal(eg: via SS) to create ROIs
2)use a feature extractor (a CNN) to extract features for the whole image first

Fast R-CNN
原始图像 -> region proposal -> region location -> feature maps(small image batches)->ROI pooling->FC->classification and localization
         -> CNN             -> feature map(whole image)
ROI pooling 做的事情就是把不同的feature maps统一成一样的大小，实现类似max-pooling/average-pooling
ROI带来的misalignment的问题：(严格意义上讲第一个misaligment不是ROI pooling产生的)
   1)bounding box coordinates mapping from original image space to feature map
   2)ROI pooling quantizations(横切一刀，竖切一刀，不能整除，两次丢失精度)
如何解决问题2)?
ROI Align:
   保留除法之后的小数位，对图像做双线性插值(具体在ROI Align中是怎么做插值的还是不太清楚，这个需要继续研究)
   cv2.resize函数中有这个参数(https://www.tutorialkart.com/opencv/python/)
效果：10* times faster then R-CNN in trainning and 150* faster in inferencing(理解应该是前向传播)

Fast R-CNN 的问题：
depends on an external region proposal method like SS. Those algrithms run on CPU and they are slow. 
In testing: Fast R-CNN takes 2.3 seconds to make a prediction in which 2 seconds are for genertting 2000 ROIs
希望网路偶可以通过深度学习直接生成一些候选框，而不是用SS方法，这样可以实现一个端到端的网络

如何解决问题?
不采用SS，改用RPN(region proposal network区域生成网络) 可以理解为一个子网络，一般训练两三轮即可

Faster R-CNN(2015-2016火了两年)
原始图像->CNN->RPN->regions-> ROI Pooling->FC->classification and localization
             ->feature maps->
             
为什么RPN具备候选框生成的能力呢???       
RPN中最重要的一个新概念anchor:
Faster R-CNN doesn't make random boundry box proposals.Instead, it preidct offsets to the top left corner of some 
reference boxes called anchors.
从概念来看:anchor主要是为了减少模型训练时需要寻找的候选框数量，对于每个location, 设置9个anchor(3组长宽高,每组包括一个正方形和横纵两个长方形)

下面这段话加深理解：
RPN在CNN提取特征之后以sliding window的方式在最后一个feature map上提取特征，每个滑动窗口中心都关联着 k个 box，
这些box就称为anchor，或者叫anchor box。这些关联的box 可以通过逆向映射对应到原图上，对应到原图上的区域就是region proposals，
不过这些region proposals都是位于同一个中心点。就是说sliding window时的window（大小固定）
是由这些原图上的不同大小和比例的 region proposals 生成的（类似于ROI池化的功能）。
实际上sliding window时每个 window 起到了一部分region proposals 的作用，
但是由于这里的sliding window的尺寸是固定的，所以不能起到多尺度，多尺寸（multiple scales and sizes ）预测的作用，
因此提出关联k个不同大小和长宽比的anchor box，这样二者结合即可起到多尺度，多尺寸预测的作用

假设一张800*600的图像，经过VGG下采样16倍后，anchor的总数量是 (800/16)(600/16)*9=17100 anchor的并集已经超出了原图像
注意：
对anchor的理解要放在原图上，而不是特征图上
对anchor的长宽设置要结合目标物体，比如人的话就要设置为细长的长方形，汽车可能要设置为水平的长方形
对anchor的大小设置要结合目标物体，比如有很多小的人脸，这是要让anchor小一些

Mask R-CNN 
在Faster R-CNN的基础上多了一个Mask FCN predictor输出层
多了实例分割：在像素层面识别目标轮廓的任务
以有7个气球的一张图像为例：
目标检测：这张图像中的这些位置上有 7 个气球(Faster R-CNN)
实例分割：这些位置上有 7 个气球，并且这些像素分别属于每个气球(Mask R-CNN)

RPN扫描anchor速度？
非常快，滑动窗口是由 RPN 的卷积过程实现的，可以使用 GPU 并行地扫描所有区域。
此外，RPN 并不会直接扫描图像，而是扫描主干特征图。这使得 RPN 可以有效地复用提取的特征，并避免重复计算。
通过这些优化手段，RPN 可以在 10ms 内完成扫描（根据引入 RPN 的 Faster R-CNN 论文中所述）。
在 Mask R-CNN 中，我们通常使用的是更高分辨率的图像以及更多的 anchor，因此扫描过程可能会更久。

-------------------------------------Single Shot Object Detectors(更加注重性能,部署到实际环境)---------------------------------

YOLO

除了滑动窗口，还有其他的目标检测算法吗？
该方法对目标的标注，需要标注目标的位置、大小、类型等信息，标注成本是很高的。但是，做目标检测是少不了这个标注工作的。
这种传统的滑动窗口目标检测方法，最大的缺点是： 窗口大小不固定，需要动态改变窗口做多次训练、预测操作，这对模型的训练是非常耗时的。

有没有办法针对一幅图像，只做一次训练呢？这就是YOLO算法要解决的问题。
解决方案： 把图像分成N*N个cells，每一个cell预测B个box,每个cell预测C类

具体的，每个小格子有5个标注信息：
bx,by,bh,bw：目标物体中心点在网格中的x-y坐标，以及目标物体的长、宽。一般将小网格长宽取1后进行标注，这样位置坐标都被转化为0~1之间的数值。
网络吴恩达版本：
pc：小网格内是否有目标物体的中心点，注意是根据中心点进行标注，横跨多个网格的物体也只标注有中心点存在的网格。
课程版本：
box confidence score:refelects how likely the box contains an object and how accurate is the bounding box
问题1：查一下是不是同一个东西???

所以YOLO的输出是(N,N, B * 5 + C)
对于所有的bounding box(N*N*B)不是所有的box都有用，设置一个confidence阈值，大于这个阈值的留下

问题2：resolution和感受野的关系？老师说分辨率越大，感受野越小？理解一下？

YOLO的github demo地址： https://github.com/pjreddie/darknet/wiki/YOLO:-Real-Time-Object-Detection


SSD (single shot --multibox-- detector)

为什么引出SSD? use multi-scale feature map and default bounding boxes to detect objects at different scale
eg:一个8*8的feature map可以识别出更小的物体

SSD流程：
image->VG19->conv->conv->conv->conv->conv(five conv layers)+conv(convolution filters)->classes + boundary boxes
缺点：convolution layers reduce spatial dimentsion and resolution
so the model above can detect large objects only , to fix that we make independent object detections from multiple feature maps
改进：把中间每一层的conv都输出到最后的 FC (在不同的conv上参数可能有不同有的4有的6)

SSD经过VGG后 38*38 a pretty large reduction from the input image, hence SSD usualy perfoms badly for small objects comparing with other 
dection methods. 我们可以通过提高原图像分辨率的方法来解决这个问题

SSD损失：
1)localization loss 
the localization loss is the mismatch between the ground truth box and the predicted boundary box 
SSD only penalizes predictions from positive mathces (首先保证是一个object 如果连个物体都不是，就不用计loss)
to get coloser to the ground truth. Negtive matches can be ignored (不是object忽略)
2)confidence loss 分类损失
the confidence loss is the loss in making a class prediction 
for every positive match prediction, we penalize the loss according to the confidence score of the corresponding class

SSD中用的两种方法：
1)hard negative mining 难例挖掘
  一般负样本远多于正样本，样本不均衡影响训练效果
  instead of using all the negatives , we sort those negatives by their calculated confidence loss
  SSD picks the negatives with the top loss and makes sure the ratio between the picked negatives and positives is at most 3:1
  this leads to a faster and more stable training
2)Data augmentation(来自论文) 这个方法主要用来弥补SSD对于比较小的物体检测困难
To make the model more robust to various input object sizes and shapes, each training image is randomly sampled by one of the following options:
– Use the entire original input image.
– Sample a patch so that the minimum jaccard overlap with the objects is 0.1, 0.3,0.5, 0.7, or 0.9.
– Randomly sample a patch.
The size of each sampled patch is [0.1, 1] of the original image size, and the aspect ratio is between 1/2 and 2. 
We keep the overlapped part of the ground truth box if the center of it is in the sampled patch. 
After the aforementioned sampling step, each sampled patch is resized to fixed size and is horizontally flipped with probability of 0.5, in addition to
applying some photo-metric distortions similar to those described in 

SSD的几个说明
1) SSD performs worse than faster R-CNN for small-scale objects 
   in SSD small objects can only be detected in higher resolution layers (left most layers)
   but those layers contains low-level features like edges or color patches, that are less infromative for classification
2) accuracy increases with the number of default boundary boxes at the cost of speed
3) multi-scale feature maps improve the detection of objects at different scale(VGG带来了feature map的上限-》后面理解下？？？)
4) design better default boundary boxes will help accuracy
5) SSD has lower localization error comparing with RCNN but more classification error dealing with similar categories
   the higher classification errors are likely because we use the same boundary box to make multiple class predictions
6) SSD512(512指的是分辨率) has better accuracy 2.5% than SSD300 but run at 22 FPS instead of 59

SSD源码位置：https://github.com/weiliu89/caffe/tree/ssd 

阅读SSD论文的笔记
1)Often detection speed for these approaches is measured in seconds per frame (SPF) and even the fastest high-accuracy detector, 
Faster R-CNN, operates at only 7 frames per second (FPS).
2)Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, 
using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from
the later stages of a network in order to perform detection at multiple scales 
3)Jaccard index,又称为Jaccard相似系数（Jaccard similarity coefficient）用于比较有限样本集之间的相似性与差异性,值越大，样本相似度越高。


YOLO-V3

YOLOV3有3个scale分别对应到不同大小的物体
第一层   stride=32 feature map比较小 但是感受野大 用来检测大物体
中间一层用来检测中等物体
最后一层 stride=8 reslution最大 感受野最小 用来检测小物体

理解：步子迈得越大->得到的feature map越小->感受野(对应到原来的区域越大)越大->所以用来检测大物体

YOLO-V2相比V3计算损失函数的时候采用的是MSE 
YOLO-V3分类损失用的交叉熵，这个比较make sense 
softmaxing classes rest on the assumption that classes are mutually exclusive
this works fine in COCO dataset (Microsoft Common Objects in Context 微软2014出资标注的数据集)
however when we have classes like Person and Women in a dataset the above assumption fails.
This is the reason why the authors of YOLO have refrained(克制 节制 避免) from softmaxing the classes
instead each class score is predicted using LR and threshold is used to predict multiple labels from an object

YOLOV3中的anchor聚类：clustering for anchor boxes:
in many problem domains, the boundary boxes ave strong patterns,
for example, in the autonomous driving, the most common boundary boxes will be cars and pedistrians（行人）at different distances
to identify the top-k boundary boxes that have the best coverage for the training data, we run K-Means clustering on the data
to locate the centroids of the top-k clusters

YOLOV3中应该用多少个anchor?
to make a decision on how many anchor boxes to use, we plot the curve IoU vs N cluster plot
the ides is that the true number of clusters is captured when the increase in the mean IoU slope 开始很大后面逐渐平缓
in this case I would say that 4 anchor boxes may be a good size 
anchor box 越多，准确率会上去但是效率会下降，折中取4个

YOLOV3中的focal loss:

focal loss 解决什么问题？ 
答：样本数量不均衡问题：两种样本不均衡 正负样本 难易样本
positive sample >>>>> negative samples (class imbalance)
easy samples >>>>> hard samples 

和交叉熵很像但是又有区别：
设置两个参数 alpha代表平衡因子 用来解决正负样本不均衡  gamma用来解决难易样本不均衡
对于那些分类已经很正确的样本，权重会下降，对于分类错误的样本，权重会升高
实际应用中 gamma=2 和 alpha=0.25的组合常用

------------------------------------------关于以上模型的一些讨论------------------------------------------------
VGG: Faster RCNN/SSD
MobileNet: Faster RCNN/R-FCN/SSD
InceptionV2 : Faster RCNN/R-FCN/SSD
Resnet : Faster RCNN/R-FCN/SSD
Inception Resnet V2 : Faster RCNN/R-FCN/SSD

The difference between detectors is narrowing 准确率和速度的权衡
single shot uses more complex designs to make it more accurate 
region based detectors streamilne the operaion to make it faster
Eventually the significant diff may not be in the basic concept of the models but on the implementation details 
1) Feature pyramid networks produces semantic rich feature maps with high resolution object spatial information to improve accuracy
https://blog.csdn.net/qq_17550379/article/details/80375874
2）complex feature extractors like ResNet and Inception ResNet are key to high accuracy if speed is not a concern
3) single shot detecotrs with light but powerful extractor like MobileNet is good for realtime processing
   in particular for less powerful mobile device
4) use batch normalization
5) experiment different feature extractors to find a good balance between speed and accuracy
   some light weight extractors make significant speed improvement with tolerable accuracy drop
6) use anchors to make boundary box predictions 
7) select anchors carefully 
8) crop images in traning to learn features in diff scales
9) at the cost of speed , higher resolution input images improves accuracy, in particular for small obj
10) fewer proposals for faster RCNN can imporve speed without to much accuracy drop
11) end to end traning with multi-task loss improves performance
12) experiment diff weights for diff losses
13) experiment atrous mode (https://www.zhihu.com/question/49630217 带洞卷积)
   it provides wider field of view at the same computational cost. it can help accuracy
14)exp the number of proposals or predictions per grid cell


------------------------------------------FPN and Anchor Free Methods Object Detection------------------------------------------------














---------------------------------------------------------其他--------------------------------------------------
番外篇：知乎上的优秀文章：
https://zhuanlan.zhihu.com/c_1178388040400302080

FPN啥关系？ 
triplet loss 
CNN是通过softmax来解决排序的问题，想想是不是有哪里不太一样？
对于某一个神经网络，我的输入是带有三个输
怎么理解embedding??
https://www.zhihu.com/question/38002635

Euclidean space 欧式空间
harmonic embeddings 谐波嵌入
off the shelf 现成的
bottleneck layer 瓶颈层 输入输出维度差距较大，就像一个瓶颈一样，上窄下宽亦或上宽下窄，深入理解下如下链接
https://blog.csdn.net/u011501388/article/details/80389164

cross level vehicle recongnition CLVR
robotics
myriad 无数的

数据扩增的包 imgaug 数据增强库 网上有对应的学习笔记

如果现在有一个车辆识别任务

method1: train convnets to predict vehicle models
training 就直接用inception v3

start with a convnet (eg.inception v3) initialized with imagenet pretrained weights
remove the lsat fully-connected layers 
extract the 2048-d features by adding a gloable average polling layer after the last convolutional layer
adding the output layer for predicting vehicle models which is a fully-connected layer

裁剪图片的时候 a better way : 因为图片很多情况下是长方形，不能直接选中中心位置，不然不准确，
所以先确定短边，然后按照比例找中间的小长方形，然后已中心位置旋转90度，两个长方形交叠的正方形为最后裁剪结果
we call this method as center-crop 
但是这样的问题是：如果每次选取中心位置，那么信息可能有丢失
为了保证每次都能训练到不同的位置，在实际操作过程中，会用padding去做一些补零的操作，然后每次随机去做移动，这样每次训练都可以得到不同的区域

进阶method2: 利用颜色数据做多任务学习
凡是神经网络有两个以上输出分支的我们call it multi-task learning
这里我们输出车型和颜色两个分支 颜色更加容易学习，所以在实际操作中可以降低权重

进阶method3：multi-task learning = triple loss + cross-entropy

在实际训练中，我们如何准备我们得negative数据呢？
比如一张图片是白色大众帕萨特，它的negative如果是红色奥迪，其实在上面的multi-task learning中
我们已经可以根据颜色分支区别出来，所以我们要选取一些hard negative的输入样本，
比如白色奥迪，这样可以更加高效的利用负样本，让模型可区分的颗粒度更细

结论：你想让模型区分哪些方面？就喂给模型什么样的数据，否则triplet loss没办法发挥作用

repression network
when calculating triplet loss , contact softmax layer
3 ways to contact: 1) multiply the matrix 2)subtrcat 3)use inception-like net

调参工程师 或者 高级 调参工程师

output: 
loss curves 
gradient norms

Architecture：
using or adapt from established networks:
   classification: AlexNET vgg resnet denseNet
   segmentation ...
data is less , then we not need to set the param too large, for small dataset can reduce the channels 
change the last layers due to real task:
1) regression task : change to FC+LR
2)...

when tuning parameters, only use a samll portion of the dataset for fast iteration 
-> because if you use lots of data to train, then every time if you change one param will need long time 
eg: make sure you model overfit curernt small portion (make the loss function can 收敛 quickly)

Class YourAwesomeDataset(torch.data.Dataset):
   def __init__(root_dir,debug=False):
    ***
    
Reading the learning curves:
Loss curves: 
6 pictrues:
1) 来回震荡太厉害,没有收敛 -》比较差的情况，要去检查数据处理阶段问题等等
2) train loss is lower than val loss -> overfit 看最后收敛的百分比，30% 以上可能就要注意 10%到20%可以酌情
   方法 L1 L2 norm dropp out 等等 
3) train loss is lower than val loss -> overfit 比起第二个 
4) train loss and val loss 相伴下降，但是类似一个斜线  最后的结果不是最好的，应该还能到一个值，收敛的斜率不平
   方法：先learning rate 大再小
5) 开始平稳，也要调大学习率
6) 梯度方向错误 不讨论

7) 8) same problem need to shuffle the original data 
9) need normlization
10) train loss lower 

checklist:
overfit on a small dataset
train on standard corpora
mean certering 
balanced dataset 
   offline: 
   online: eg: batch_size = 256 sample from diff classes (if 10) when 11 then sample from class 1,2,3 .. then it will make sure all
           the classes are balanced in the batch
   loss function: control weight 
simplify your model
weight init 
pretrianed model 
learning rate
cross-entropy loss 
regularization loss  first close it -> make the model overfit -> then open it 
action function 
gradient clipping-> make a threshold for the weight , if over, then pull it back 

iteration vs epoch 
iteration -> epoch / batch size 
epoch -> iteration to the whole data set

工业界想牺牲一定的准确性保证高效率

Xception:
depthwise conv on seperate channel 

flir forward  looking infra-red 前视红外雷达


VGG19 https://www.jianshu.com/p/6aa903648ec5 了解下
aspect ratio: 图像的宽高比
